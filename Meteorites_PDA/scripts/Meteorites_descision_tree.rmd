---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(modelr)
library(yardstick)
library(here)
library(caret)
meteor <-  read_csv(here("clean_data/landings_CLEAN_additional_class_data.csv"))
meteor

shuffle_index <- sample(1:nrow(meteor))

meteor_set <- meteor[shuffle_index, ] %>%
  filter(fall %in% c("Fell", "Found")) %>%
  mutate(class = as.factor(class)) %>%
  mutate(fall_flag = as.factor(fall)) %>% 
  select(fall_flag, mass_g, year, lattitude, longditude, class) %>%
  na.omit()
  
meteor_set
```

```{r}
meteor_set %>%
  ggplot(aes(x = fall_flag)) +
  geom_bar()

## very unbalanced data set
```


```{r}
## caret stratifies by default so should be a better option than a random split
## the balance of the data means stratification is much more important
## stratification ensures that the test and training data have an equal ratio of positives and negatives
test_index <- createDataPartition(meteor_set$fall_flag, p = .7, list = FALSE)

train  <- slice(meteor_set, test_index) ## 4.3 creation
test  <- slice(meteor_set, -test_index) ## 4.2 creation

## training data us used for the model to identify the trend in the data.
## testing data is used to validate the end models performance by ensuring the model works on new data its never seen before.
## a big risk is over fitting where a model will learn the training data with a very high accuracy.
## however, this inhibits a models ability to generalize and thus the performance on real data becomes very poor.

## in this split, 70% of the data is uses as training data leaving 30% as test to measure the models performance.

nrow(test)
nrow(train)

```

```{r}
## 4.3 training data is used to fit the model
meteor_fit <- rpart(fall_flag ~ mass_g + year, data = train, method = 'class', cp = 0.01) 
plotcp(meteor_fit)
rpart.plot(meteor_fit, yesno = 2, faclen = 6, fallen.leaves = TRUE, extra = 101)
```
4.3 / 4.6 training data tree generated
the accuracy for this model on the training data is 87.93%
the root node of this decision tree uses year as a predictor 
this shows that year is one of the most important predictors for this model
this first branching from the root divides the data into a 21/79 percentage split.
the larger portion of this data (79%) is sent to a terminal node immediately, this terminal node has a classification accuracy on the test data of 92%
this means that the first branch reliant only on the year being greater or less than 1938 explains a massive portion of the data (79%) with a fairly high accuracy (92%)
the model also shows the probability that a meteorite will be classed as fallen based on all predictors is 0.85 which is very high.
this shows that the model has a bias toward found meteorites.
this is not unexpected due to the data imbalance, meaning the data itself has a bias toward found.
a very notable trend within this model is the fact that larger dates appear to be predictors of a meteorite being found while older dates appear to predict a higher probability of a fall classification. this mirrors what we know about the data as a whole.
furthermore, meteorites with lower mass values seem to have a higher probability to be classed as fall.
this would make sense as a smaller meteorite would be more difficult to find on earth once landed or could burn up completely upon atmospheric entry


4.2 / 4.6 test dataset confusion matrix

```{r}
test_pred <- test %>%
                 add_predictions(meteor_fit, type = 'class')


conf_mat <- test_pred %>%
              conf_mat(truth = fall_flag, estimate = pred)
conf_mat
```
this matrix shows that model has a tenancy to predict that a meteorite was found.
out of all 1429 values in the test data set the model predicted (66 + 1180) = 1246 correctly 
this gives the model a 87.19% accuracy on the test data set, very similar to the training data (87.93%)
this is not unexpected as the data is very unbalanced and the model will create a higher success rate by erring on the side of found.
this can be seen as the number of fell classed as found is over double than what the model predicts correctly.
the years appear to be higher in the tree and thus the larger predictors.
found meteorites can also not be found again and there is a finite supply on earth.
as interest increases, the number found also increases making them rarer and more difficult to find.
interest in  meteorites has been influenced by modern technology and as such our ability to find meteorites has increased.
this may explain why so many meteorites have been found in the years after 1936

